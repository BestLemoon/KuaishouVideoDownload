#!/usr/bin/env python3
"""
GitHub Actions è‡ªåŠ¨åšå®¢æ–‡ç« ç”Ÿæˆè„šæœ¬ - å…³é”®è¯é©±åŠ¨ç‰ˆæœ¬
"""
import os
import requests
import time
import random
import re
from datetime import datetime
from google.generativeai import configure, GenerativeModel
from supabase import create_client, Client
import uuid
from typing import List, Dict, Any

# ç¯å¢ƒå˜é‡é…ç½®
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
UNSPLASH_ACCESS_KEY = os.getenv('UNSPLASH_ACCESS_KEY')
SITE_URL = os.getenv('NEXT_PUBLIC_WEB_URL', 'https://kuaishou-video-download.com')

# åˆå§‹åŒ–æœåŠ¡
configure(api_key=GEMINI_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

def get_unsplash_image(query="kuaishou video"):
    """ä»Unsplashè·å–å›¾ç‰‡"""
    try:
        if not UNSPLASH_ACCESS_KEY:
            return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"
        
        headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
        response = requests.get(
            f"https://api.unsplash.com/search/photos?query={query}&per_page=30&orientation=landscape",
            headers=headers,
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('results'):
                photo = random.choice(data['results'])
                return f"{photo['urls']['regular']}?w=800&q=80"
                
        return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"
    except Exception as e:
        print(f"è·å–Unsplashå›¾ç‰‡å¤±è´¥: {e}")
        return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"

def generate_seed_keywords(language: str, count: int = 8) -> List[str]:
    """ç”Ÿæˆç§å­å…³é”®è¯"""
    model = GenerativeModel("gemini-2.5-flash-preview-05-20")
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„SEOå…³é”®è¯ç ”ç©¶ä¸“å®¶ï¼Œä¸“æ³¨äºå¿«æ‰‹è§†é¢‘ä¸‹è½½ç›¸å…³çš„å…³é”®è¯ç ”ç©¶ã€‚

## ä»»åŠ¡
è¯·ä¸ºKuaishouVideoDownloadï¼ˆå¿«æ‰‹è§†é¢‘ä¸‹è½½å™¨ï¼‰ç”Ÿæˆ{count}ä¸ªé«˜ä»·å€¼çš„ç§å­å…³é”®è¯ã€‚

## å…³é”®è¯ç±»å‹è¦æ±‚
è¯·ç”Ÿæˆä»¥ä¸‹ç±»å‹çš„å…³é”®è¯ï¼š
1. ğŸ” æœç´¢å‹å…³é”®è¯ï¼ˆç”¨æˆ·ç›´æ¥æœç´¢éœ€æ±‚ï¼‰
2. ğŸ“± è®¾å¤‡ç›¸å…³å…³é”®è¯ï¼ˆiPhone, Android, mobileç­‰ï¼‰
3. ğŸ“Š åŠŸèƒ½å‹å…³é”®è¯ï¼ˆbatch download, HD qualityç­‰ï¼‰
4. ğŸ’¡ è§£å†³æ–¹æ¡ˆå‹å…³é”®è¯ï¼ˆhow to, best wayç­‰ï¼‰
5. ğŸŒ ç«å“å’Œæ¯”è¾ƒå…³é”®è¯ï¼ˆvs, alternativeç­‰ï¼‰

## ç›®æ ‡è¯­è¨€
{language}

## è¾“å‡ºè¦æ±‚
- ç›´æ¥è¾“å‡º{count}ä¸ªå…³é”®è¯
- æ¯è¡Œä¸€ä¸ªå…³é”®è¯
- ä¸åŒ…å«ç¼–å·æˆ–ç¬¦å·
- å…³é”®è¯åº”è¯¥å…·æœ‰æœç´¢ä»·å€¼
- é¿å…è¿‡äºå®½æ³›æˆ–è¿‡äºç»†åˆ†çš„è¯

è¯·å¼€å§‹ç”Ÿæˆï¼š

(å”¯ä¸€æ€§æ ‡è¯†: {int(time.time())})"""

    try:
        result = model.generate_content(prompt)
        if not result.text:
            raise ValueError("AIæœªèƒ½ç”Ÿæˆç§å­å…³é”®è¯")
        
        # è§£æå…³é”®è¯
        keywords = []
        lines = result.text.strip().split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('-'):
                # æ¸…ç†å¯èƒ½çš„ç¼–å·å’Œç¬¦å·
                keyword = re.sub(r'^\d+\.?\s*', '', line)
                keyword = re.sub(r'^[â€¢\-\*]\s*', '', keyword)
                keyword = keyword.strip('"').strip("'").strip()
                if keyword and len(keyword) > 2:
                    keywords.append(keyword)
        
        # ç¡®ä¿è¿”å›æŒ‡å®šæ•°é‡
        keywords = keywords[:count]
        print(f"âœ… ç”Ÿæˆäº†{len(keywords)}ä¸ª{language}ç§å­å…³é”®è¯")
        return keywords if keywords else get_default_seed_keywords(language, count)
        
    except Exception as e:
        print(f"âŒ ç§å­å…³é”®è¯ç”Ÿæˆå¤±è´¥: {e}")
        return get_default_seed_keywords(language, count)

def get_default_seed_keywords(language: str, count: int) -> List[str]:
    """è·å–é»˜è®¤ç§å­å…³é”®è¯"""
    if "chinese" in language.lower() or "ä¸­æ–‡" in language:
        default_keywords = [
            "å¿«æ‰‹è§†é¢‘ä¸‹è½½",
            "å¿«æ‰‹è§†é¢‘ä¿å­˜",
            "å¿«æ‰‹çŸ­è§†é¢‘ä¸‹è½½",
            "å¿«æ‰‹ä¸‹è½½å™¨",
            "å¿«æ‰‹è§†é¢‘ä¸‹è½½å·¥å…·",
            "å¿«æ‰‹è§†é¢‘ä¿å­˜",
            "å¿«æ‰‹çŸ­è§†é¢‘",
            "ä¸‹è½½å¿«æ‰‹è§†é¢‘"
        ]
    else:
        default_keywords = [
            "kuaishou video downloader",
            "download kuaishou video",
            "kuaishou video download",
            "kuaishou downloader",
            "save kuaishou video",
            "kuaishou video saver",
            "download from kuaishou",
            "kuaishou media download"
        ]
    
    return default_keywords[:count]

def get_google_suggestions(keyword: str, max_suggestions: int = 8) -> List[str]:
    """ä½¿ç”¨Googleè‡ªåŠ¨å®ŒæˆAPIè·å–å…³é”®è¯å»ºè®®"""
    try:
        url = "http://suggestqueries.google.com/complete/search"
        params = {
            'client': 'firefox',
            'q': keyword
        }
        
        print(f"ğŸ” è·å–'{keyword}'çš„Googleè‡ªåŠ¨å®Œæˆå»ºè®®...")
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        suggestions_data = response.json()
        if len(suggestions_data) >= 2 and isinstance(suggestions_data[1], list):
            suggestions = suggestions_data[1][:max_suggestions]
            
            # è¿‡æ»¤å’Œæ¸…ç†å»ºè®®
            filtered_suggestions = []
            for suggestion in suggestions:
                if isinstance(suggestion, str) and suggestion.strip():
                    # ç§»é™¤Unicodeè½¬ä¹‰å­—ç¬¦
                    clean_suggestion = suggestion.encode().decode('unicode_escape')
                    filtered_suggestions.append(clean_suggestion)
            
            print(f"âœ… è·å–åˆ°{len(filtered_suggestions)}ä¸ªè‡ªåŠ¨å®Œæˆå»ºè®®")
            return filtered_suggestions
        else:
            print("âš ï¸ æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆçš„è‡ªåŠ¨å®Œæˆå»ºè®®")
            return []
            
    except Exception as e:
        print(f"âŒ è·å–Googleè‡ªåŠ¨å®Œæˆå»ºè®®å¤±è´¥: {e}")
        return []

def expand_keywords_with_google(seed_keywords: List[str], max_per_keyword: int = 6) -> Dict[str, List[str]]:
    """ä½¿ç”¨Googleè‡ªåŠ¨å®Œæˆæ‰©å±•ç§å­å…³é”®è¯"""
    expanded_keywords = {}
    
    print(f"\nğŸš€ å¼€å§‹æ‰©å±•{len(seed_keywords)}ä¸ªç§å­å…³é”®è¯...")
    
    for i, keyword in enumerate(seed_keywords, 1):
        print(f"\nè¿›åº¦: {i}/{len(seed_keywords)} - å¤„ç†: {keyword}")
        
        suggestions = get_google_suggestions(keyword, max_per_keyword)
        if suggestions:
            expanded_keywords[keyword] = suggestions
        
        # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        if i < len(seed_keywords):
            time.sleep(1)  # 1ç§’å»¶è¿Ÿ
    
    return expanded_keywords

def generate_categorized_topics_by_keywords(expanded_keywords: Dict[str, List[str]], language: str) -> Dict[str, List[str]]:
    """æ ¹æ®æ‰©å±•çš„å…³é”®è¯ï¼ŒæŒ‰åˆ†ç±»ç”Ÿæˆæ–‡ç« é¢˜ç›®"""
    model = GenerativeModel("gemini-2.5-flash-preview-05-20")
    
    # å°†æ‰€æœ‰å…³é”®è¯åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨ç”¨äºAIåˆ†æ
    all_keywords = []
    for seed_keyword, suggestions in expanded_keywords.items():
        all_keywords.append(seed_keyword)
        all_keywords.extend(suggestions)
    
    # å»é‡
    unique_keywords = list(set(all_keywords))
    keywords_text = '\n'.join(f"- {kw}" for kw in unique_keywords[:50])  # é™åˆ¶å…³é”®è¯æ•°é‡
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„SEOå†…å®¹ç­–ç•¥å¸ˆï¼Œä¸“æ³¨äºKuaishouVideoDownloadï¼ˆå¿«æ‰‹è§†é¢‘ä¸‹è½½å™¨ï¼‰ç›¸å…³çš„å†…å®¹åˆ›ä½œã€‚

## ä»»åŠ¡
åŸºäºä»¥ä¸‹æ‰©å±•å…³é”®è¯ï¼ŒæŒ‰ç…§æŒ‡å®šç±»åˆ«ç”Ÿæˆæ–‡ç« é¢˜ç›®å»ºè®®ã€‚

## å¯ç”¨å…³é”®è¯ï¼š
{keywords_text}

## æ–‡ç« ç±»åˆ«è¦æ±‚ï¼š

### ğŸ” æœç´¢å‹å…³é”®è¯æ–‡ç« ï¼ˆæ¯æ—¥3ç¯‡ï¼‰
- é’ˆå¯¹ç”¨æˆ·æœç´¢æ„å›¾ï¼Œé•¿å°¾å…³é”®è¯ä¸ºä¸»
- å¦‚"how to download Kuaishou video on iPhone"
- è§£å†³å…·ä½“ç”¨æˆ·é—®é¢˜çš„æ–‡ç« 
éœ€è¦ç”Ÿæˆï¼š3ä¸ªé¢˜ç›®

### ğŸ“˜ æ•™ç¨‹å‹/åˆ—è¡¨å‹æ–‡ç« ï¼ˆæ¯æ—¥1ç¯‡ï¼‰
- å¢åŠ åˆ†äº«ç‡ï¼Œé€‚åˆå†…éƒ¨é“¾æ¥
- å¦‚"Top 5 Kuaishou Video Downloaders 2025"
- æ¯”è¾ƒã€æ’è¡Œã€å®Œæ•´æŒ‡å—ç±»å‹
éœ€è¦ç”Ÿæˆï¼š1ä¸ªé¢˜ç›®

### ğŸŒ åŠŸèƒ½ä»‹ç»æ–‡ç« ï¼ˆæ¯æ—¥1ç¯‡ï¼‰
- ä»‹ç»å¿«æ‰‹è§†é¢‘ä¸‹è½½çš„å„ç§åŠŸèƒ½å’ŒæŠ€å·§
- æå‡ç”¨æˆ·ä½“éªŒå’Œäº§å“è®¤çŸ¥
éœ€è¦ç”Ÿæˆï¼š1ä¸ªé¢˜ç›®

## è¯­è¨€è¦æ±‚
{language}

## è¾“å‡ºæ ¼å¼
è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

===SEARCH_KEYWORDS_START===
[3ä¸ªæœç´¢å‹æ–‡ç« é¢˜ç›®ï¼Œæ¯è¡Œä¸€ä¸ª]
===SEARCH_KEYWORDS_END===

===TUTORIAL_LISTS_START===
[1ä¸ªæ•™ç¨‹å‹/åˆ—è¡¨å‹æ–‡ç« é¢˜ç›®]
===TUTORIAL_LISTS_END===

===FEATURE_CONTENT_START===
[1ä¸ªåŠŸèƒ½ä»‹ç»æ–‡ç« é¢˜ç›®]
===FEATURE_CONTENT_END===

è¯·ç¡®ä¿æ‰€æœ‰é¢˜ç›®éƒ½ä¸æä¾›çš„å…³é”®è¯ç›¸å…³ï¼Œå…·æœ‰SEOä»·å€¼ï¼š

(å”¯ä¸€æ€§æ ‡è¯†: {int(time.time())})"""

    try:
        result = model.generate_content(prompt)
        if not result.text:
            raise ValueError("AIæœªèƒ½ç”Ÿæˆåˆ†ç±»æ–‡ç« é¢˜ç›®")
        
        # è§£æåˆ†ç±»é¢˜ç›®
        categories = {
            'search_keywords': extract_category_topics(result.text, "===SEARCH_KEYWORDS_START===", "===SEARCH_KEYWORDS_END==="),
            'tutorial_lists': extract_category_topics(result.text, "===TUTORIAL_LISTS_START===", "===TUTORIAL_LISTS_END==="),
            'feature_content': extract_category_topics(result.text, "===FEATURE_CONTENT_START===", "===FEATURE_CONTENT_END===")
        }
        
        print(f"âœ… æˆåŠŸç”Ÿæˆåˆ†ç±»æ–‡ç« é¢˜ç›®:")
        for category, topics in categories.items():
            print(f"   {category}: {len(topics)}ä¸ªé¢˜ç›®")
        
        return categories
        
    except Exception as e:
        print(f"âŒ åˆ†ç±»æ–‡ç« é¢˜ç›®ç”Ÿæˆå¤±è´¥: {e}")
        return get_default_category_topics(language)

def extract_category_topics(content: str, start_delimiter: str, end_delimiter: str) -> List[str]:
    """ä»åˆ†éš”ç¬¦ä¸­æå–åˆ†ç±»é¢˜ç›®"""
    try:
        start_idx = content.find(start_delimiter)
        end_idx = content.find(end_delimiter)
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            start_idx += len(start_delimiter)
            section_content = content[start_idx:end_idx].strip()
            
            # è§£æé¢˜ç›®
            topics = []
            lines = section_content.split('\n')
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('==='):
                    # æ¸…ç†ç¼–å·å’Œç¬¦å·
                    topic = re.sub(r'^\d+\.?\s*', '', line)
                    topic = re.sub(r'^[â€¢\-\*]\s*', '', topic)
                    topic = topic.strip()
                    if topic:
                        topics.append(topic)
            
            return topics
        
        return []
        
    except Exception:
        return []

def get_default_category_topics(language: str) -> Dict[str, List[str]]:
    """è·å–é»˜è®¤åˆ†ç±»é¢˜ç›®"""
    if "chinese" in language.lower() or "ä¸­æ–‡" in language:
        return {
            'search_keywords': [
                "å¦‚ä½•åœ¨iPhoneä¸Šä¸‹è½½å¿«æ‰‹è§†é¢‘",
                "å¿«æ‰‹è§†é¢‘ä¸‹è½½å™¨å“ªä¸ªæœ€å¥½ç”¨",
                "å…è´¹ä¸‹è½½å¿«æ‰‹è§†é¢‘çš„æ–¹æ³•"
            ],
            'tutorial_lists': [
                "2025å¹´æœ€ä½³å¿«æ‰‹è§†é¢‘ä¸‹è½½å·¥å…·TOP5"
            ],
            'feature_content': [
                "å¿«æ‰‹è§†é¢‘ä¸‹è½½åŠŸèƒ½è¯¦è§£"
            ]
        }
    else:
        return {
            'search_keywords': [
                "How to download Kuaishou videos on iPhone",
                "Best Kuaishou video downloader 2024",
                "Free Kuaishou video download methods"
            ],
            'tutorial_lists': [
                "Top 5 Kuaishou Video Downloaders 2025"
            ],
            'feature_content': [
                "Kuaishou Video Download Features Explained"
            ]
        }

def build_keywords_context(expanded_keywords: Dict[str, List[str]]) -> str:
    """æ„å»ºå…³é”®è¯ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
    context_lines = []
    context_lines.append("ç›¸å…³å…³é”®è¯é›†åˆ:")
    
    for seed, suggestions in expanded_keywords.items():
        context_lines.append(f"â€¢ {seed}")
        for suggestion in suggestions[:5]:  # æ¯ä¸ªç§å­è¯æœ€å¤š5ä¸ªå»ºè®®
            context_lines.append(f"  - {suggestion}")
    
    return '\n'.join(context_lines)

def extract_delimiter_content(text, start_delimiter, end_delimiter):
    """æå–åˆ†éš”ç¬¦ä¹‹é—´çš„å†…å®¹"""
    start_index = text.find(start_delimiter)
    if start_index == -1:
        return None
    start_index += len(start_delimiter)
    
    end_index = text.find(end_delimiter, start_index)
    if end_index == -1:
        return None
        
    return text[start_index:end_index].strip()

def generate_slug(title):
    """ç”ŸæˆURLå‹å¥½çš„slug"""
    import re
    slug = title.lower()
    slug = re.sub(r'[^\w\s-]', '', slug)
    slug = re.sub(r'[-\s]+', '-', slug)
    return slug.strip('-')

def generate_unique_slug(base_slug, locale):
    """ç”Ÿæˆå”¯ä¸€çš„slug"""
    slug = base_slug
    counter = 1
    
    while True:
        result = supabase.table("posts").select("slug").eq("slug", slug).eq("locale", locale).execute()
        if not result.data:
            break
        slug = f"{base_slug}-{counter}"
        counter += 1
        
    return slug

def generate_article(topic, language, locale, keywords_context=""):
    """ç”Ÿæˆå•ç¯‡æ–‡ç« """
    try:
        print(f"æ­£åœ¨ç”Ÿæˆ{language}æ–‡ç« : {topic}")
        
        # è·å–ç°æœ‰æ–‡ç« ä½œä¸ºå†…é“¾å‚è€ƒ
        existing_posts = supabase.table("posts").select("title, slug, locale").eq("status", "online").eq("locale", locale).limit(10).execute()
        
        internal_links_text = ""
        if existing_posts.data:
            if locale == "en":
                internal_links_text = "\n## Existing Articles (for internal linking):\n"
                for post in existing_posts.data:
                    url = f"{SITE_URL}/posts/{post['slug']}"
                    internal_links_text += f"- [{post['title']}]({url})\n"
            else:
                internal_links_text = "\n## ç°æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆç”¨äºå†…é“¾å‚è€ƒï¼‰ï¼š\n"
                for post in existing_posts.data:
                    url = f"{SITE_URL}/zh/posts/{post['slug']}"
                    internal_links_text += f"- [{post['title']}]({url})\n"

        model = GenerativeModel("gemini-2.5-flash-preview-05-20")
        
        # æ„å»ºå…³é”®è¯ä¸Šä¸‹æ–‡
        keywords_section = ""
        if keywords_context:
            keywords_section = f"""

## å…³é”®è¯ä¼˜åŒ–æŒ‡å¯¼
åŸºäºä»¥ä¸‹ç›¸å…³å…³é”®è¯ä¼˜åŒ–ä½ çš„å†…å®¹ï¼š
{keywords_context}

**å…³é”®è¯ä½¿ç”¨è¦æ±‚ï¼š**
- è‡ªç„¶åœ°å°†è¿™äº›å…³é”®è¯èå…¥æ–‡ç« ä¸­
- åœ¨æ ‡é¢˜ã€å°æ ‡é¢˜å’Œæ­£æ–‡ä¸­åˆç†åˆ†å¸ƒå…³é”®è¯
- ç¡®ä¿å…³é”®è¯ä½¿ç”¨ä¸å½±å“å†…å®¹çš„è‡ªç„¶æ€§å’Œå¯è¯»æ€§
- ä¼˜å…ˆä½¿ç”¨é•¿å°¾å…³é”®è¯å’Œè¯­ä¹‰ç›¸å…³çš„è¯æ±‡"""
        
        if locale == "en":
            prompt = f"""You are a professional SEO content creator specializing in KuaishouVideoDownload (Kuaishou video downloader) related content.

## Task
Please create a high-quality SEO blog article for this topic: {topic}

## Requirements
- Article length: 1000-1500 words
- Language: English
- Natural, fluent writing style that avoids AI-generated traces
- Use Markdown format
- Include proper heading structure (H1, H2, H3)
- Must include at least 3 internal links to our existing related articles
- Include 2-3 high-quality external links (to authoritative websites)
- SEO optimized with naturally integrated relevant keywords

{internal_links_text}

{keywords_section}

## Internal Link Requirements
- Must naturally insert at least 3 links to the above existing articles within the content
- Internal links should be relevant to the article content and naturally integrated into paragraphs
- Use descriptive anchor text, not just "click here"
- Link format: [anchor text](URL)

## External Link Requirements
- Include 2-3 links to authoritative websites
- External links should be related to Kuaishou, video downloading, social media
- Add appropriate context for external links

## Output Format
Use the following delimiter format:

===TITLE_START===
[SEO-optimized title, max 60 characters]
===TITLE_END===

===SLUG_START===
[URL-friendly slug]
===SLUG_END===

===DESCRIPTION_START===
[Meta description, 150-160 characters, engaging summary]
===DESCRIPTION_END===

===CONTENT_START===
[Complete article content in Markdown format, must include at least 3 internal links and 2-3 external links]
===CONTENT_END===

Please generate natural, fluent content that avoids obvious AI-generated traces:

(Internal note for uniqueness: {int(time.time())})"""
        else:
            prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„SEOæ–‡ç« åˆ›ä½œè€…ï¼Œä¸“æ³¨äº KuaishouVideoDownloadï¼ˆå¿«æ‰‹è§†é¢‘ä¸‹è½½å™¨ï¼‰ç›¸å…³å†…å®¹åˆ›ä½œã€‚

## ä»»åŠ¡
è¯·ä¸ºä»¥ä¸‹é¢˜ç›®åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„SEOåšå®¢æ–‡ç« ï¼š{topic}

## è¦æ±‚
- æ–‡ç« é•¿åº¦ï¼š1000-1500å­—
- è¯­è¨€ï¼šä¸­æ–‡
- è‡ªç„¶æµç•…çš„å†™ä½œé£æ ¼ï¼Œé¿å…AIç”Ÿæˆçš„ç—•è¿¹
- ä½¿ç”¨Markdownæ ¼å¼
- åŒ…å«åˆé€‚çš„æ ‡é¢˜ç»“æ„ï¼ˆH1ã€H2ã€H3ï¼‰
- å¿…é¡»åŒ…å«è‡³å°‘3ä¸ªå†…éƒ¨é“¾æ¥åˆ°æˆ‘ä»¬ç°æœ‰çš„ç›¸å…³æ–‡ç« 
- åŒ…å«2-3ä¸ªé«˜è´¨é‡çš„å¤–éƒ¨é“¾æ¥ï¼ˆæŒ‡å‘æƒå¨ç½‘ç«™ï¼‰
- SEOä¼˜åŒ–ï¼Œè‡ªç„¶èå…¥ç›¸å…³å…³é”®è¯

{internal_links_text}

{keywords_section}

## å†…é“¾è¦æ±‚
- å¿…é¡»åœ¨å†…å®¹ä¸­è‡ªç„¶æ’å…¥è‡³å°‘3ä¸ªæŒ‡å‘ä¸Šè¿°ç°æœ‰æ–‡ç« çš„é“¾æ¥
- å†…é“¾åº”ä¸æ–‡ç« å†…å®¹ç›¸å…³ï¼Œè‡ªç„¶èå…¥åˆ°æ®µè½ä¸­
- ä½¿ç”¨æè¿°æ€§é”šæ–‡æœ¬ï¼Œä¸è¦åªæ˜¯"ç‚¹å‡»è¿™é‡Œ"
- é“¾æ¥æ ¼å¼ï¼š[é”šæ–‡æœ¬](URL)

## å¤–é“¾è¦æ±‚
- åŒ…å«2-3ä¸ªæŒ‡å‘æƒå¨ç½‘ç«™çš„é“¾æ¥
- å¤–é“¾åº”ä¸å¿«æ‰‹ã€è§†é¢‘ä¸‹è½½ã€ç¤¾äº¤åª’ä½“ç›¸å…³
- ä¸ºå¤–é“¾æ·»åŠ é€‚å½“çš„ä¸Šä¸‹æ–‡

## è¾“å‡ºæ ¼å¼
ä½¿ç”¨ä»¥ä¸‹åˆ†éš”ç¬¦æ ¼å¼ï¼š

===TITLE_START===
[SEOä¼˜åŒ–çš„æ ‡é¢˜ï¼Œæœ€å¤š60ä¸ªå­—ç¬¦]
===TITLE_END===

===SLUG_START===
[URLå‹å¥½çš„slug]
===SLUG_END===

===DESCRIPTION_START===
[å…ƒæè¿°ï¼Œ150-160å­—ç¬¦ï¼Œå¸å¼•äººçš„æ‘˜è¦]
===DESCRIPTION_END===

===CONTENT_START===
[å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ŒMarkdownæ ¼å¼ï¼Œå¿…é¡»åŒ…å«è‡³å°‘3ä¸ªå†…é“¾å’Œ2-3ä¸ªå¤–é“¾]
===CONTENT_END===

è¯·ç”Ÿæˆè‡ªç„¶ã€æµç•…çš„å†…å®¹ï¼Œé¿å…æ˜æ˜¾çš„AIç”Ÿæˆç—•è¿¹ï¼š

(å†…éƒ¨å”¯ä¸€æ€§æ ‡è¯†: {int(time.time())})"""

        result = model.generate_content(prompt)
        text = result.text
        
        if not text:
            raise Exception("AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå†…å®¹")

        # è§£æç”Ÿæˆçš„å†…å®¹
        title = extract_delimiter_content(text, "===TITLE_START===", "===TITLE_END===") or topic
        slug = extract_delimiter_content(text, "===SLUG_START===", "===SLUG_END===") or generate_slug(title)
        description = extract_delimiter_content(text, "===DESCRIPTION_START===", "===DESCRIPTION_END===") or f"å…³äº{title}çš„è¯¦ç»†æŒ‡å—"
        content = extract_delimiter_content(text, "===CONTENT_START===", "===CONTENT_END===") or text

        # ç”Ÿæˆå”¯ä¸€slug
        final_slug = generate_unique_slug(slug, locale)

        # è·å–å°é¢å›¾ç‰‡
        cover_url = get_unsplash_image("kuaishou video")

        # ä¸ºæ–‡ç« æ·»åŠ éšæœºçš„æ—¶é—´åç§»ï¼Œè®©å‘å¸ƒæ—¶é—´æ›´è‡ªç„¶
        publish_time = datetime.now()
        random_hours_back = random.randint(1, 72)
        random_minutes_back = random.randint(1, 60)
        publish_time = publish_time.replace(hour=max(0, publish_time.hour - random_hours_back % 24))
        publish_time = publish_time.replace(minute=max(0, publish_time.minute - random_minutes_back % 60))

        # æ’å…¥åˆ°æ•°æ®åº“
        post_uuid = str(uuid.uuid4())
        insert_data = {
            "uuid": post_uuid,
            "slug": final_slug,
            "title": title,
            "description": description,
            "content": content,
            "cover_url": cover_url,
            "created_at": publish_time.isoformat(),
            "updated_at": publish_time.isoformat(),
            "status": "online",
            "locale": locale,
            "author_name": "KuaishouVideoDownload Team",
            "author_avatar_url": "https://www.kuaishou-video-download.com/logo.png"
        }

        result = supabase.table("posts").insert(insert_data).execute()
        
        if result.data:
            print(f"âœ… {language}æ–‡ç« ç”ŸæˆæˆåŠŸ: {title}")
            return {
                "success": True,
                "topic": topic,
                "title": title,
                "uuid": post_uuid,
                "slug": final_slug,
                "cover_url": cover_url,
            }
        else:
            raise Exception("æ•°æ®åº“æ’å…¥å¤±è´¥")

    except Exception as e:
        print(f"âŒ {language}æ–‡ç« ç”Ÿæˆå¤±è´¥ '{topic}': {e}")
        return {
            "success": False,
            "topic": topic,
            "error": str(e),
        }

def generate_keyword_driven_articles(language: str, locale: str) -> Dict[str, Any]:
    """å…³é”®è¯é©±åŠ¨çš„æ–‡ç« ç”Ÿæˆæµç¨‹"""
    try:
        print(f"\nğŸ¯ å¼€å§‹{language}å…³é”®è¯é©±åŠ¨çš„å†…å®¹ç”Ÿæˆæµç¨‹...")
        
        # æ­¥éª¤1: ç”Ÿæˆç§å­å…³é”®è¯
        print(f"\nğŸ“Š æ­¥éª¤1: ç”Ÿæˆ{language}ç§å­å…³é”®è¯")
        seed_keywords = generate_seed_keywords(language, 6)
        
        if not seed_keywords:
            print(f"âŒ {language}ç§å­å…³é”®è¯ç”Ÿæˆå¤±è´¥")
            return {"success": 0, "failure": 0, "topics": [], "results": []}
        
        print(f"ğŸ”‘ {language}ç§å­å…³é”®è¯:")
        for i, keyword in enumerate(seed_keywords, 1):
            print(f"   {i}. {keyword}")
        
        # æ­¥éª¤2: ä½¿ç”¨Googleè‡ªåŠ¨å®Œæˆæ‰©å±•å…³é”®è¯
        print(f"\nğŸ” æ­¥éª¤2: æ‰©å±•{language}å…³é”®è¯")
        expanded_keywords = expand_keywords_with_google(seed_keywords, 5)
        
        print(f"\nğŸ“ˆ {language}æ‰©å±•åçš„å…³é”®è¯é›†åˆ:")
        total_keywords = 0
        for seed, suggestions in expanded_keywords.items():
            print(f"   ğŸŒ± {seed}: {len(suggestions)}ä¸ªå»ºè®®")
            total_keywords += len(suggestions)
        print(f"   æ€»è®¡: {len(seed_keywords)}ä¸ªç§å­å…³é”®è¯ â†’ {total_keywords}ä¸ªæ‰©å±•å…³é”®è¯")
        
        # æ­¥éª¤3: åŸºäºå…³é”®è¯ç”Ÿæˆåˆ†ç±»æ–‡ç« é¢˜ç›®
        print(f"\nğŸ“ æ­¥éª¤3: ç”Ÿæˆ{language}åˆ†ç±»æ–‡ç« é¢˜ç›®")
        categorized_topics = generate_categorized_topics_by_keywords(expanded_keywords, language)
        
        print(f"\nğŸ“š {language}ç”Ÿæˆçš„åˆ†ç±»æ–‡ç« é¢˜ç›®:")
        all_topics = []
        for category, topics in categorized_topics.items():
            print(f"   ğŸ“‚ {category}: {len(topics)}ä¸ªé¢˜ç›®")
            for topic in topics:
                print(f"      â€¢ {topic}")
                all_topics.append((category, topic))
        
        # æ­¥éª¤4: æ‰§è¡Œæ–‡ç« ç”Ÿæˆ
        print(f"\nğŸš€ æ­¥éª¤4: å¼€å§‹ç”Ÿæˆ{language}æ–‡ç« ...")
        
        # æ„å»ºå…³é”®è¯ä¸Šä¸‹æ–‡
        keywords_context = build_keywords_context(expanded_keywords)
        
        results = []
        success_count = 0
        failure_count = 0
        
        for category, topic in all_topics:
            topic_to_use = topic
            
            print(f"\nğŸ“ ç”Ÿæˆæ–‡ç« : {topic_to_use} (åˆ†ç±»: {category})")
            result = generate_article(topic_to_use, language, locale, keywords_context)
            results.append(result)
            
            if result["success"]:
                success_count += 1
                print(f"âœ… æˆåŠŸ: {result['title']}")
            else:
                failure_count += 1
                print(f"âŒ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(3)
        
        print(f"\nğŸ‰ {language}å…³é”®è¯é©±åŠ¨ç”Ÿæˆå®Œæˆ!")
        print(f"   ğŸ“Š ç§å­å…³é”®è¯: {len(seed_keywords)} ä¸ª")
        print(f"   ğŸ” æ‰©å±•å…³é”®è¯: {total_keywords} ä¸ª")
        print(f"   ğŸ“ æˆåŠŸç”Ÿæˆæ–‡ç« : {success_count} ç¯‡")
        print(f"   âŒ å¤±è´¥: {failure_count} ç¯‡")
        
        return {
            "success": success_count,
            "failure": failure_count,
            "topics": [topic for _, topic in all_topics],
            "results": results,
            "seed_keywords": seed_keywords,
            "expanded_keywords": expanded_keywords,
            "categorized_topics": categorized_topics
        }
        
    except Exception as e:
        print(f"âŒ {language}å…³é”®è¯é©±åŠ¨ç”Ÿæˆå¤±è´¥: {e}")
        return {"success": 0, "failure": 0, "topics": [], "results": []}

def main():
    """ä¸»å‡½æ•° - åªç”Ÿæˆè‹±æ–‡æ–‡ç« """
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥è‹±æ–‡æ–‡ç« ç”Ÿæˆä»»åŠ¡ï¼ˆ5ç¯‡ï¼‰")
    print("=" * 60)

    # åªç”Ÿæˆè‹±æ–‡æ–‡ç« 
    print("\nğŸ‡ºğŸ‡¸ å¼€å§‹è‹±æ–‡å…³é”®è¯é©±åŠ¨ç”Ÿæˆ...")
    results = generate_keyword_driven_articles("English", "en")

    print(f"\nğŸ‰ æ¯æ—¥è‹±æ–‡æ–‡ç« ç”Ÿæˆä»»åŠ¡å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   ğŸ‡ºğŸ‡¸ è‹±æ–‡: æˆåŠŸ {results['success']} ç¯‡ï¼Œå¤±è´¥ {results['failure']} ç¯‡")

    # è®°å½•ä»»åŠ¡æ‰§è¡Œæ—¥å¿—åˆ°æ•°æ®åº“
    try:
        log_data = {
            "execution_date": datetime.now().date().isoformat(),
            "chinese_success": 0,
            "chinese_failure": 0,
            "english_success": results["success"],
            "english_failure": results["failure"],
            "total_success": results["success"],
            "total_failure": results["failure"],
            "generation_method": "keyword_driven_english_only",
            "created_at": datetime.now().isoformat()
        }
        supabase.table("auto_generation_logs").insert(log_data).execute()
        print(f"âœ… æ‰§è¡Œæ—¥å¿—å·²è®°å½•åˆ°æ•°æ®åº“")
    except Exception as log_error:
        print(f"âš ï¸ æ—¥å¿—è®°å½•å¤±è´¥ï¼ˆä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼‰: {log_error}")

    return results

if __name__ == "__main__":
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "keywords":
            # å…³é”®è¯é©±åŠ¨æ¨¡å¼
            language = sys.argv[2] if len(sys.argv) > 2 else "both"
            
            print("ğŸš€ å¯åŠ¨å…³é”®è¯é©±åŠ¨æ–‡ç« ç”Ÿæˆæ¨¡å¼")
            print(f"ğŸŒ ç›®æ ‡è¯­è¨€: {language}")
            
            if language.lower() in ["chinese", "zh", "ä¸­æ–‡"]:
                print("\nğŸ‡¨ğŸ‡³ ä»…ç”Ÿæˆä¸­æ–‡å†…å®¹...")
                result = generate_keyword_driven_articles("Chinese (Simplified)", "zh")
                print(f"âœ… ä¸­æ–‡ç”Ÿæˆå®Œæˆ: æˆåŠŸ {result['success']} ç¯‡")
            elif language.lower() in ["english", "en", "è‹±æ–‡"]:
                print("\nğŸ‡ºğŸ‡¸ ä»…ç”Ÿæˆè‹±æ–‡å†…å®¹...")
                result = generate_keyword_driven_articles("English", "en")
                print(f"âœ… è‹±æ–‡ç”Ÿæˆå®Œæˆ: æˆåŠŸ {result['success']} ç¯‡")
            else:
                # é»˜è®¤ç”ŸæˆåŒè¯­
                main()
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
            print("ğŸ’¡ å¯ç”¨å‘½ä»¤:")
            print("   python auto_generate_articles.py keywords [language]")
            print("   language å¯é€‰å€¼: chinese/zh/ä¸­æ–‡, english/en/è‹±æ–‡, both(é»˜è®¤)")
    else:
        # é»˜è®¤æ‰§è¡Œå…³é”®è¯é©±åŠ¨çš„åŒè¯­ç”Ÿæˆ
        main() 