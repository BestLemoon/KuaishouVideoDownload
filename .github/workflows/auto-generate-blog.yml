name: Auto Generate Blog Articles

on:
  schedule:
    # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ (UTCæ—¶é—´)
    - cron: '0 2 * * *'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  generate-articles:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Python dependencies
      run: |
        pip install -r requirements-github-actions.txt

    - name: Create auto-generate script
      run: |
        cat > auto_generate_articles.py << 'EOF'
        import os
        import requests
        import time
        import random
        from datetime import datetime
        from google.generativeai import configure, GenerativeModel
        from supabase import create_client, Client
        import uuid

        # çŽ¯å¢ƒå˜é‡é…ç½®
        GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
        SUPABASE_URL = os.getenv('SUPABASE_URL')
        SUPABASE_SERVICE_ROLE_KEY = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        UNSPLASH_ACCESS_KEY = os.getenv('UNSPLASH_ACCESS_KEY')
        SITE_URL = os.getenv('NEXT_PUBLIC_WEB_URL', 'https://twitterdown.com')

        # åˆå§‹åŒ–æœåŠ¡
        configure(api_key=GEMINI_API_KEY)
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

        def get_unsplash_image(query="twitter"):
            """ä»ŽUnsplashèŽ·å–å›¾ç‰‡"""
            try:
                if not UNSPLASH_ACCESS_KEY:
                    return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"
                
                headers = {"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"}
                response = requests.get(
                    f"https://api.unsplash.com/search/photos?query={query}&per_page=30&orientation=landscape",
                    headers=headers,
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('results'):
                        photo = random.choice(data['results'])
                        return f"{photo['urls']['regular']}?w=800&q=80"
                        
                return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"
            except Exception as e:
                print(f"èŽ·å–Unsplashå›¾ç‰‡å¤±è´¥: {e}")
                return "https://images.unsplash.com/photo-1611605698335-8b1569810432?w=800&q=80"

        def generate_topics(language, count=5):
            """ç”Ÿæˆæ–‡ç« é¢˜ç›®"""
            model = GenerativeModel("gemini-2.5-flash-preview-05-20")
            
            prompt = f"""
            ä½œä¸ºä¸€ä½ä¸“ä¸šçš„SEOå†…å®¹ç­–åˆ’å¸ˆï¼Œè¯·ä¸ºTwitterDownï¼ˆTwitterè§†é¢‘ä¸‹è½½å·¥å…·ï¼‰ç”Ÿæˆ{count}ä¸ªé«˜è´¨é‡çš„åšå®¢æ–‡ç« é¢˜ç›®ã€‚

            è¦æ±‚ï¼š
            - è¯­è¨€ï¼š{language}
            - æ¯ä¸ªé¢˜ç›®éƒ½è¦ä¸ŽTwitterã€è§†é¢‘ä¸‹è½½ã€ç¤¾äº¤åª’ä½“ç›¸å…³
            - é¢˜ç›®è¦æœ‰æœç´¢ä»·å€¼å’Œç”¨æˆ·å…³æ³¨åº¦
            - é¿å…é‡å¤å’Œç›¸ä¼¼çš„é¢˜ç›®
            - ç¡®ä¿é¢˜ç›®é€‚åˆSEOä¼˜åŒ–

            è¯·ç›´æŽ¥è¾“å‡º{count}ä¸ªé¢˜ç›®ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ï¼š
            """
            
            try:
                result = model.generate_content(prompt)
                topics = [line.strip() for line in result.text.strip().split('\n') if line.strip()]
                return topics[:count]
            except Exception as e:
                print(f"ç”Ÿæˆ{language}é¢˜ç›®å¤±è´¥: {e}")
                return []

        def extract_delimiter_content(text, start_delimiter, end_delimiter):
            """æå–åˆ†éš”ç¬¦ä¹‹é—´çš„å†…å®¹"""
            start_index = text.find(start_delimiter)
            if start_index == -1:
                return None
            start_index += len(start_delimiter)
            
            end_index = text.find(end_delimiter, start_index)
            if end_index == -1:
                return None
                
            return text[start_index:end_index].strip()

        def generate_slug(title):
            """ç”ŸæˆURLå‹å¥½çš„slug"""
            import re
            slug = title.lower()
            slug = re.sub(r'[^\w\s-]', '', slug)
            slug = re.sub(r'[-\s]+', '-', slug)
            return slug.strip('-')

        def generate_unique_slug(base_slug, locale):
            """ç”Ÿæˆå”¯ä¸€çš„slug"""
            slug = base_slug
            counter = 1
            
            while True:
                result = supabase.table("posts").select("slug").eq("slug", slug).eq("locale", locale).execute()
                if not result.data:
                    break
                slug = f"{base_slug}-{counter}"
                counter += 1
                
            return slug

        def generate_article(topic, language, locale):
            """ç”Ÿæˆå•ç¯‡æ–‡ç« """
            try:
                print(f"æ­£åœ¨ç”Ÿæˆ{language}æ–‡ç« : {topic}")
                
                # èŽ·å–çŽ°æœ‰æ–‡ç« ä½œä¸ºå†…é“¾å‚è€ƒ
                existing_posts = supabase.table("posts").select("title, slug, locale").eq("status", "online").eq("locale", locale).limit(10).execute()
                
                internal_links_text = ""
                if existing_posts.data:
                    if locale == "en":
                        internal_links_text = "\n## Existing Articles (for internal linking):\n"
                        for post in existing_posts.data:
                            url = f"{SITE_URL}/posts/{post['slug']}"
                            internal_links_text += f"- [{post['title']}]({url})\n"
                    else:
                        internal_links_text = "\n## çŽ°æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆç”¨äºŽå†…é“¾å‚è€ƒï¼‰ï¼š\n"
                        for post in existing_posts.data:
                            url = f"{SITE_URL}/zh/posts/{post['slug']}"
                            internal_links_text += f"- [{post['title']}]({url})\n"

                model = GenerativeModel("gemini-2.5-flash-preview-05-20")
                
                if locale == "en":
                    prompt = f"""You are a professional SEO content creator specializing in TwitterDown (Twitter video downloader) related content.

## Task
Please create a high-quality SEO blog article for this topic: {topic}

## Requirements
- Article length: 1000-1500 words
- Language: English
- Natural, fluent writing style that avoids AI-generated traces
- Use Markdown format
- Include proper heading structure (H1, H2, H3)
- **Must include at least 3 internal links** to our existing related articles
- Include 2-3 high-quality external links (to authoritative websites)
- SEO optimized with naturally integrated relevant keywords

{internal_links_text}

## Internal Link Requirements
- Must naturally insert at least 3 links to the above existing articles within the content
- Internal links should be relevant to the article content and naturally integrated into paragraphs
- Use descriptive anchor text, not just "click here"
- Link format: [anchor text](URL)

## External Link Requirements
- Include 2-3 links to authoritative websites
- External links should be related to Twitter, video downloading, social media
- Add appropriate context for external links

## Output Format
Use the following delimiter format:

===TITLE_START===
[SEO-optimized title, max 60 characters]
===TITLE_END===

===SLUG_START===
[URL-friendly slug]
===SLUG_END===

===DESCRIPTION_START===
[Meta description, 150-160 characters, engaging summary]
===DESCRIPTION_END===

===CONTENT_START===
[Complete article content in Markdown format, must include at least 3 internal links and 2-3 external links]
===CONTENT_END===

Please generate natural, fluent content that avoids obvious AI-generated traces:

(Internal note for uniqueness: {int(time.time())})"""
                else:
                    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„SEOæ–‡ç« åˆ›ä½œè€…ï¼Œä¸“æ³¨äºŽ TwitterDownï¼ˆTwitterè§†é¢‘ä¸‹è½½å™¨ï¼‰ç›¸å…³å†…å®¹åˆ›ä½œã€‚

## ä»»åŠ¡
è¯·ä¸ºä»¥ä¸‹é¢˜ç›®åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„SEOåšå®¢æ–‡ç« ï¼š{topic}

## è¦æ±‚
- æ–‡ç« é•¿åº¦ï¼š1000-1500å­—
- è¯­è¨€ï¼šä¸­æ–‡
- è‡ªç„¶æµç•…çš„å†™ä½œé£Žæ ¼ï¼Œé¿å…AIç”Ÿæˆçš„ç—•è¿¹
- ä½¿ç”¨Markdownæ ¼å¼
- åŒ…å«åˆé€‚çš„æ ‡é¢˜ç»“æž„ï¼ˆH1ã€H2ã€H3ï¼‰
- **å¿…é¡»åŒ…å«è‡³å°‘3ä¸ªå†…éƒ¨é“¾æŽ¥**åˆ°æˆ‘ä»¬çŽ°æœ‰çš„ç›¸å…³æ–‡ç« 
- åŒ…å«2-3ä¸ªé«˜è´¨é‡çš„å¤–éƒ¨é“¾æŽ¥ï¼ˆæŒ‡å‘æƒå¨ç½‘ç«™ï¼‰
- SEOä¼˜åŒ–ï¼Œè‡ªç„¶èžå…¥ç›¸å…³å…³é”®è¯

{internal_links_text}

## å†…é“¾è¦æ±‚
- å¿…é¡»åœ¨å†…å®¹ä¸­è‡ªç„¶æ’å…¥è‡³å°‘3ä¸ªæŒ‡å‘ä¸Šè¿°çŽ°æœ‰æ–‡ç« çš„é“¾æŽ¥
- å†…é“¾åº”ä¸Žæ–‡ç« å†…å®¹ç›¸å…³ï¼Œè‡ªç„¶èžå…¥åˆ°æ®µè½ä¸­
- ä½¿ç”¨æè¿°æ€§é”šæ–‡æœ¬ï¼Œä¸è¦åªæ˜¯"ç‚¹å‡»è¿™é‡Œ"
- é“¾æŽ¥æ ¼å¼ï¼š[é”šæ–‡æœ¬](URL)

## å¤–é“¾è¦æ±‚
- åŒ…å«2-3ä¸ªæŒ‡å‘æƒå¨ç½‘ç«™çš„é“¾æŽ¥
- å¤–é“¾åº”ä¸ŽTwitterã€è§†é¢‘ä¸‹è½½ã€ç¤¾äº¤åª’ä½“ç›¸å…³
- ä¸ºå¤–é“¾æ·»åŠ é€‚å½“çš„ä¸Šä¸‹æ–‡

## è¾“å‡ºæ ¼å¼
ä½¿ç”¨ä»¥ä¸‹åˆ†éš”ç¬¦æ ¼å¼ï¼š

===TITLE_START===
[SEOä¼˜åŒ–çš„æ ‡é¢˜ï¼Œæœ€å¤š60ä¸ªå­—ç¬¦]
===TITLE_END===

===SLUG_START===
[URLå‹å¥½çš„slug]
===SLUG_END===

===DESCRIPTION_START===
[å…ƒæè¿°ï¼Œ150-160å­—ç¬¦ï¼Œå¸å¼•äººçš„æ‘˜è¦]
===DESCRIPTION_END===

===CONTENT_START===
[å®Œæ•´çš„æ–‡ç« å†…å®¹ï¼ŒMarkdownæ ¼å¼ï¼Œå¿…é¡»åŒ…å«è‡³å°‘3ä¸ªå†…é“¾å’Œ2-3ä¸ªå¤–é“¾]
===CONTENT_END===

è¯·ç”Ÿæˆè‡ªç„¶ã€æµç•…çš„å†…å®¹ï¼Œé¿å…æ˜Žæ˜¾çš„AIç”Ÿæˆç—•è¿¹ï¼š

(å†…éƒ¨å”¯ä¸€æ€§æ ‡è¯†: {int(time.time())})"""

                result = model.generate_content(prompt)
                text = result.text
                
                if not text:
                    raise Exception("AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå†…å®¹")

                # è§£æžç”Ÿæˆçš„å†…å®¹
                title = extract_delimiter_content(text, "===TITLE_START===", "===TITLE_END===") or topic
                slug = extract_delimiter_content(text, "===SLUG_START===", "===SLUG_END===") or generate_slug(title)
                description = extract_delimiter_content(text, "===DESCRIPTION_START===", "===DESCRIPTION_END===") or f"å…³äºŽ{title}çš„è¯¦ç»†æŒ‡å—"
                content = extract_delimiter_content(text, "===CONTENT_START===", "===CONTENT_END===") or text

                # ç”Ÿæˆå”¯ä¸€slug
                final_slug = generate_unique_slug(slug, locale)

                # èŽ·å–å°é¢å›¾ç‰‡
                cover_url = get_unsplash_image("twitter")

                # ä¸ºæ–‡ç« æ·»åŠ éšæœºçš„æ—¶é—´åç§»ï¼Œè®©å‘å¸ƒæ—¶é—´æ›´è‡ªç„¶
                publish_time = datetime.now()
                random_hours_back = random.randint(1, 72)
                random_minutes_back = random.randint(1, 60)
                publish_time = publish_time.replace(hour=max(0, publish_time.hour - random_hours_back % 24))
                publish_time = publish_time.replace(minute=max(0, publish_time.minute - random_minutes_back % 60))

                # æ’å…¥åˆ°æ•°æ®åº“
                post_uuid = str(uuid.uuid4())
                insert_data = {
                    "uuid": post_uuid,
                    "slug": final_slug,
                    "title": title,
                    "description": description,
                    "content": content,
                    "cover_url": cover_url,
                    "created_at": publish_time.isoformat(),
                    "updated_at": publish_time.isoformat(),
                    "status": "online",
                    "locale": locale,
                    "author_name": "TwitterDown Team",
                    "author_avatar_url": "https://www.twitterdown.com/logo.png"
                }

                result = supabase.table("posts").insert(insert_data).execute()
                
                if result.data:
                    print(f"âœ… {language}æ–‡ç« ç”ŸæˆæˆåŠŸ: {title}")
                    return {
                        "success": True,
                        "topic": topic,
                        "title": title,
                        "uuid": post_uuid,
                        "slug": final_slug,
                        "cover_url": cover_url,
                    }
                else:
                    raise Exception("æ•°æ®åº“æ’å…¥å¤±è´¥")

            except Exception as e:
                print(f"âŒ {language}æ–‡ç« ç”Ÿæˆå¤±è´¥ '{topic}': {e}")
                return {
                    "success": False,
                    "topic": topic,
                    "error": str(e),
                }

        def main():
            """ä¸»å‡½æ•°"""
            print("ðŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥è‡ªåŠ¨æ–‡ç« ç”Ÿæˆä»»åŠ¡")
            
            results = {
                "chinese": {"success": 0, "failure": 0, "topics": [], "results": []},
                "english": {"success": 0, "failure": 0, "topics": [], "results": []}
            }

            # ç”Ÿæˆä¸­æ–‡æ–‡ç« 
            try:
                print("ðŸ“ ç”Ÿæˆä¸­æ–‡é¢˜ç›®...")
                chinese_topics = generate_topics("Chinese", 5)
                results["chinese"]["topics"] = chinese_topics
                
                print("ðŸ‡¨ðŸ‡³ å¼€å§‹ç”Ÿæˆä¸­æ–‡æ–‡ç« ...")
                for topic in chinese_topics:
                    result = generate_article(topic, "Chinese", "zh")
                    results["chinese"]["results"].append(result)
                    
                    if result["success"]:
                        results["chinese"]["success"] += 1
                    else:
                        results["chinese"]["failure"] += 1
                    
                    # å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(2)
                    
                print(f"âœ… ä¸­æ–‡æ–‡ç« ç”Ÿæˆå®Œæˆï¼šæˆåŠŸ {results['chinese']['success']} ç¯‡ï¼Œå¤±è´¥ {results['chinese']['failure']} ç¯‡")
            except Exception as e:
                print(f"âŒ ä¸­æ–‡æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
                results["chinese"]["failure"] = 5

            # ç­‰å¾…é¿å…APIé™åˆ¶
            time.sleep(5)

            # ç”Ÿæˆè‹±æ–‡æ–‡ç« 
            try:
                print("ðŸ“ ç”Ÿæˆè‹±æ–‡é¢˜ç›®...")
                english_topics = generate_topics("English", 5)
                results["english"]["topics"] = english_topics
                
                print("ðŸ‡ºðŸ‡¸ å¼€å§‹ç”Ÿæˆè‹±æ–‡æ–‡ç« ...")
                for topic in english_topics:
                    result = generate_article(topic, "English", "en")
                    results["english"]["results"].append(result)
                    
                    if result["success"]:
                        results["english"]["success"] += 1
                    else:
                        results["english"]["failure"] += 1
                    
                    # å»¶è¿Ÿé¿å…APIé™åˆ¶
                    time.sleep(2)
                    
                print(f"âœ… è‹±æ–‡æ–‡ç« ç”Ÿæˆå®Œæˆï¼šæˆåŠŸ {results['english']['success']} ç¯‡ï¼Œå¤±è´¥ {results['english']['failure']} ç¯‡")
            except Exception as e:
                print(f"âŒ è‹±æ–‡æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
                results["english"]["failure"] = 5

            total_success = results["chinese"]["success"] + results["english"]["success"]
            total_failure = results["chinese"]["failure"] + results["english"]["failure"]

            print(f"ðŸŽ‰ æ¯æ—¥æ–‡ç« ç”Ÿæˆä»»åŠ¡å®Œæˆï¼šæ€»è®¡æˆåŠŸ {total_success} ç¯‡ï¼Œå¤±è´¥ {total_failure} ç¯‡")

            # è®°å½•ä»»åŠ¡æ‰§è¡Œæ—¥å¿—åˆ°æ•°æ®åº“
            try:
                log_data = {
                    "execution_date": datetime.now().date().isoformat(),
                    "chinese_success": results["chinese"]["success"],
                    "chinese_failure": results["chinese"]["failure"],
                    "english_success": results["english"]["success"],
                    "english_failure": results["english"]["failure"],
                    "total_success": total_success,
                    "total_failure": total_failure,
                    "created_at": datetime.now().isoformat()
                }
                supabase.table("auto_generation_logs").insert(log_data).execute()
            except Exception as log_error:
                print(f"æ—¥å¿—è®°å½•å¤±è´¥ï¼ˆä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼‰: {log_error}")

            return results

        if __name__ == "__main__":
            main()
        EOF

    - name: Run article generation
      env:
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        UNSPLASH_ACCESS_KEY: ${{ secrets.UNSPLASH_ACCESS_KEY }}
        NEXT_PUBLIC_WEB_URL: ${{ secrets.NEXT_PUBLIC_WEB_URL }}
      run: |
        python auto_generate_articles.py 